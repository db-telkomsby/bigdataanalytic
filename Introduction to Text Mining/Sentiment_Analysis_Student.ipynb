{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/db-telkomsby/bigdataanalytic/blob/main/Sentiment_Analysis_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYmp622rh-tc"
      },
      "source": [
        "##**1. Import Library**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H07VVcmgPm3O"
      },
      "source": [
        "* VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.\n",
        "Source : https://github.com/cjhutto/vaderSentiment\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZC7sIOvhLkD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e64d1274-f05b-410a-8ebe-fb375c7b6cd5"
      },
      "source": [
        "# Install Library\n",
        "!pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2023.7.22)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj-iCWSNQ4KB"
      },
      "source": [
        "* Pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
        "* NLTK is a leading platform for building Python programs to work with human language data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akevYOvYhMqD"
      },
      "source": [
        "# Import Library\n",
        "import pandas as pd\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCGAzlQqh5_m"
      },
      "source": [
        "##**2. Import Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s4No_aAdnc5"
      },
      "source": [
        "# Import Data from Github\n",
        "url = 'https://raw.githubusercontent.com/mrbarokah/Python/master/dataset/Text_GeneralMotor.csv'\n",
        "df = pd.read_csv(url, sep=',',)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTiM5zNweBng"
      },
      "source": [
        "# Import Data from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uz7p18EefJL"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Text_GeneralMotor.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dYWIffzhb4S"
      },
      "source": [
        "# Import Data if Normal\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Text_GeneralMotor.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoNKFGqKRWvQ"
      },
      "source": [
        "# Import Normal Data if UnicodeError Occured\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Text_GeneralMotor.csv\", encoding = \"ISO-8859-1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7pG-XNthyOo"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huk6nZDjh15j"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce6eIbGKlvl_"
      },
      "source": [
        "#**3. PreProcessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvMQYxnNBXAI"
      },
      "source": [
        "###a. Remove Duplicate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p93BaJ8_i9_"
      },
      "source": [
        "# Remove Duplicate Row from Table\n",
        "df = df.drop_duplicates()\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxHScdi48YC4"
      },
      "source": [
        "###b. RemoveURL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBvj60dL8WIa"
      },
      "source": [
        "# Remove Duplicate from Selected Column\n",
        "df['text'] = df['text'].str.replace('http\\S+|www.\\S+', '', case=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erMmoXQbwxK0"
      },
      "source": [
        "###c. LowerCasing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzi6mWuvulb-"
      },
      "source": [
        "# Merubah keseluruhan kalimat di kolom yang dipilih menjadi huruf kecil\n",
        "df['text'] = df['text'].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8pT1YApwrth"
      },
      "source": [
        "df.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GR5iPwH_wsU"
      },
      "source": [
        "###d. RemoveUsername (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMZRgD1h_0Sh"
      },
      "source": [
        "# Menghilangkan kata yang diawali oleh simbol @ pada kolom tertentu\n",
        "df['text'] = df['text'].str.replace('@[^\\s]+','', case=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7GNk15qxhLJ"
      },
      "source": [
        "###e. Tokenize (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PtudKVrzUdF"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve-_m0d_xj8P"
      },
      "source": [
        "#Testing\n",
        "example_text = df.iloc[0]\n",
        "print(nltk.word_tokenize(example_text['text']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ianpkdFyzQ3"
      },
      "source": [
        "def identify_tokens(row):\n",
        "    text = row['text']\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # taken only words (not punctuation)\n",
        "    token_words = [w for w in tokens if w.isalpha()]\n",
        "    return token_words\n",
        "\n",
        "df['text'] = df.apply(identify_tokens, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1D8eJsuzB-Z"
      },
      "source": [
        "df.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv0m1ExWzg87"
      },
      "source": [
        "###f. Stemming (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyKjBwLhzjFe"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemming = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zz2ArPmzlht"
      },
      "source": [
        "my_list = ['frightening', 'frightened', 'frightens']\n",
        "print ([stemming.stem(word) for word in my_list])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4KljGvi0gj1"
      },
      "source": [
        "def stem_list(row):\n",
        "    my_list = row['text']\n",
        "    stemmed_list = [stemming.stem(word) for word in my_list]\n",
        "    return (stemmed_list)\n",
        "\n",
        "df['stemmed_words'] = df.apply(stem_list, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JofG8_AP0ss2"
      },
      "source": [
        "df.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHW4Og7VrrSO"
      },
      "source": [
        "###g. Stopwords (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOHzEbFA1SvX"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stops = set(stopwords.words(\"english\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na6chvUW1YHl"
      },
      "source": [
        "def remove_stops(row):\n",
        "    text = row['text']\n",
        "    meaningful_words = [w for w in text if not w in stops]\n",
        "    return (meaningful_words)\n",
        "\n",
        "df['text'] = df.apply(remove_stops, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Wy5D-4vR9ZV"
      },
      "source": [
        "#Joining Text\n",
        "df['text'] = df['text'].str.join(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T925fp31h18"
      },
      "source": [
        "print(df['text'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkOE2fChzFFy"
      },
      "source": [
        "###h. Special Character (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxrqp4be7YkH"
      },
      "source": [
        "import string\n",
        "\n",
        "printable = set(string.printable)\n",
        "\n",
        "def remove_spec_chars(in_str):\n",
        "    return ''.join([c for c in in_str if c in printable])\n",
        "\n",
        "df['text'].apply(remove_spec_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKXeNt32lnoM"
      },
      "source": [
        "#**4. Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9i3qzQsDPc6"
      },
      "source": [
        "#Change Title to String\n",
        "df['text'] = df['text'].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEa3xwALhD2T"
      },
      "source": [
        "# Import library for Text Analytics\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dISYmHJxhHzF"
      },
      "source": [
        "# Sentiment Analysis\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "listy = []\n",
        "for index, row in df.iterrows():\n",
        "  df['text']\n",
        "  ss = sid.polarity_scores(row['text'])\n",
        "  listy.append(ss)\n",
        "\n",
        "se = pd.Series(listy)\n",
        "df['polarity'] = se.values\n",
        "display(df.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QCKCoqHz7nn"
      },
      "source": [
        "###a. Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvN0c3J9zvPT"
      },
      "source": [
        "# Pie Chart\n",
        "import matplotlib.pyplot as plt\n",
        "labels = ['negative', 'neutral', 'positive']\n",
        "sizes  = [ss['neg'], ss['neu'], ss['pos']]\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW_YHFkszUxI"
      },
      "source": [
        "###b. Save to CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2scLhNwqhS-"
      },
      "source": [
        "df.to_csv('Output_File.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
